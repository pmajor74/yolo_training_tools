"""HTML report generator for benchmark results."""

from datetime import datetime
from pathlib import Path
from typing import Dict, Any


class BenchmarkReportGenerator:
    """Generate HTML reports for benchmark results."""
    
    def generate_report(self, results: Dict[str, Any]) -> str:
        """Generate a comprehensive HTML report."""
        metadata = results.get('metadata', {})
        timestamp = metadata.get('timestamp', datetime.now().isoformat())
        
        html = f"""<!DOCTYPE html>
<html>
<head>
    <title>YOLO Benchmark Report</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        {self._get_css_styles()}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>YOLO Model Benchmark Report</h1>
            <div class="timestamp">Generated: {datetime.fromisoformat(timestamp).strftime('%Y-%m-%d %H:%M:%S')}</div>
        </div>
        
        <div class="content">
            {self._generate_summary_section(results)}
            {self._generate_configuration_section(metadata)}
            {self._generate_overall_metrics_section(results)}
            {self._generate_per_class_section(results)}
            {self._generate_problem_images_section(results)}
            {self._generate_per_image_section(results)}
            {self._generate_recommendations_section(results)}
        </div>
        
        <div class="footer">
            <p>Generated by YOLO Training Tools Benchmarking System</p>
        </div>
    </div>
</body>
</html>"""
        return html
        
    def _get_css_styles(self) -> str:
        """Get CSS styles for the report."""
        return """
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }
        .container {
            max-width: 1600px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }
        h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        }
        .timestamp {
            opacity: 0.9;
            font-size: 0.9em;
        }
        .content {
            padding: 30px;
        }
        .summary {
            background: #f8f9fa;
            border-left: 4px solid #667eea;
            padding: 20px;
            margin-bottom: 30px;
            border-radius: 5px;
        }
        .summary h2 {
            color: #333;
            margin-bottom: 15px;
        }
        .summary-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin-top: 15px;
        }
        .summary-item {
            background: white;
            padding: 15px;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .summary-item strong {
            color: #667eea;
            display: block;
            margin-bottom: 5px;
        }
        .metric-value {
            font-size: 1.5em;
            font-weight: bold;
            color: #333;
        }
        .section {
            margin-bottom: 40px;
        }
        .section-header {
            background: linear-gradient(135deg, #84fab0 0%, #8fd3f4 100%);
            padding: 15px 20px;
            margin-bottom: 20px;
            border-radius: 5px;
            font-size: 1.3em;
            font-weight: bold;
            color: #333;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
        }
        th {
            background: #667eea;
            color: white;
            padding: 12px;
            text-align: left;
        }
        td {
            padding: 10px;
            border-bottom: 1px solid #e0e0e0;
        }
        tr:nth-child(even) {
            background: #f8f9fa;
        }
        tr:hover {
            background: #e8eaf6;
        }
        .good {
            color: #4caf50;
            font-weight: bold;
        }
        .warning {
            color: #ff9800;
            font-weight: bold;
        }
        .bad {
            color: #f44336;
            font-weight: bold;
        }
        .metric-card {
            background: white;
            border: 1px solid #e0e0e0;
            border-radius: 10px;
            padding: 20px;
            margin-bottom: 20px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .metric-card h3 {
            color: #667eea;
            margin-bottom: 15px;
        }
        .progress-bar {
            width: 100%;
            height: 30px;
            background: #e0e0e0;
            border-radius: 15px;
            overflow: hidden;
            margin: 10px 0;
        }
        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, #667eea, #764ba2);
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
            transition: width 0.3s ease;
        }
        .recommendation {
            background: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: 15px;
            margin: 10px 0;
            border-radius: 5px;
        }
        .recommendation.good {
            background: #e8f5e9;
            border-left-color: #4caf50;
        }
        .recommendation.critical {
            background: #ffebee;
            border-left-color: #f44336;
        }
        .footer {
            background: #f8f9fa;
            text-align: center;
            padding: 20px;
            color: #666;
            border-top: 1px solid #e0e0e0;
        }
        .collapsible {
            cursor: pointer;
            padding: 10px;
            background: #f0f0f0;
            border: none;
            text-align: left;
            width: 100%;
            font-size: 1.1em;
            margin-top: 10px;
        }
        .collapsible:hover {
            background: #e0e0e0;
        }
        .collapsible-content {
            padding: 0 18px;
            display: none;
            overflow: hidden;
            background-color: #f9f9f9;
        }
        """
        
    def _generate_summary_section(self, results: Dict) -> str:
        """Generate the summary section."""
        precision = results.get('precision', 0)
        recall = results.get('recall', 0)
        f1 = results.get('f1_score', 0)
        map50 = results.get('map_50', 0)
        
        # Determine performance level
        if f1 >= 0.8:
            performance = "Excellent"
            performance_class = "good"
        elif f1 >= 0.6:
            performance = "Good"
            performance_class = "good"
        elif f1 >= 0.4:
            performance = "Fair"
            performance_class = "warning"
        else:
            performance = "Poor"
            performance_class = "bad"
            
        return f"""
        <div class="summary">
            <h2>Executive Summary</h2>
            <p>Model performance evaluation on {results.get('total_images', 0)} test images 
            with {results.get('total_ground_truth', 0)} ground truth annotations.</p>
            
            <div class="summary-grid">
                <div class="summary-item">
                    <strong>Overall Performance</strong>
                    <div class="metric-value {performance_class}">{performance}</div>
                </div>
                <div class="summary-item">
                    <strong>F1 Score</strong>
                    <div class="metric-value">{f1:.2%}</div>
                </div>
                <div class="summary-item">
                    <strong>mAP@0.5</strong>
                    <div class="metric-value">{map50:.2%}</div>
                </div>
                <div class="summary-item">
                    <strong>Avg Inference</strong>
                    <div class="metric-value">{results.get('avg_inference_time', 0):.3f}s</div>
                </div>
            </div>
        </div>
        """
        
    def _generate_configuration_section(self, metadata: Dict) -> str:
        """Generate the configuration section."""
        model_name = Path(metadata.get('model_path', 'Unknown')).name
        test_folder = Path(metadata.get('test_folder', 'Unknown')).name
        
        return f"""
        <div class="section">
            <div class="section-header">Test Configuration</div>
            <table>
                <tr>
                    <td><strong>Model:</strong></td>
                    <td>{model_name}</td>
                </tr>
                <tr>
                    <td><strong>Test Dataset:</strong></td>
                    <td>{test_folder}</td>
                </tr>
                <tr>
                    <td><strong>Number of Images:</strong></td>
                    <td>{metadata.get('num_images', 0)}</td>
                </tr>
                <tr>
                    <td><strong>Number of Classes:</strong></td>
                    <td>{len(metadata.get('class_names', {}))}</td>
                </tr>
                <tr>
                    <td><strong>Confidence Threshold:</strong></td>
                    <td>{metadata.get('conf_threshold', 0.25)}</td>
                </tr>
                <tr>
                    <td><strong>IoU Threshold:</strong></td>
                    <td>{metadata.get('iou_threshold', 0.45)}</td>
                </tr>
            </table>
        </div>
        """
        
    def _generate_overall_metrics_section(self, results: Dict) -> str:
        """Generate the overall metrics section."""
        tp = results.get('true_positives', 0)
        fp = results.get('false_positives', 0)
        fn = results.get('false_negatives', 0)
        
        precision = results.get('precision', 0)
        recall = results.get('recall', 0)
        f1 = results.get('f1_score', 0)
        
        return f"""
        <div class="section">
            <div class="section-header">Overall Performance Metrics</div>
            
            <div class="metric-card">
                <h3>Detection Statistics</h3>
                <table>
                    <tr>
                        <th>Metric</th>
                        <th>Count</th>
                        <th>Percentage</th>
                    </tr>
                    <tr>
                        <td>True Positives</td>
                        <td class="good">{tp}</td>
                        <td>{tp / (tp + fp + fn) * 100 if (tp + fp + fn) > 0 else 0:.1f}%</td>
                    </tr>
                    <tr>
                        <td>False Positives</td>
                        <td class="warning">{fp}</td>
                        <td>{fp / (tp + fp + fn) * 100 if (tp + fp + fn) > 0 else 0:.1f}%</td>
                    </tr>
                    <tr>
                        <td>False Negatives</td>
                        <td class="bad">{fn}</td>
                        <td>{fn / (tp + fp + fn) * 100 if (tp + fp + fn) > 0 else 0:.1f}%</td>
                    </tr>
                </table>
            </div>
            
            <div class="metric-card">
                <h3>Key Performance Indicators</h3>
                
                <div style="margin: 20px 0;">
                    <strong>Precision: {precision:.2%}</strong>
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {precision*100}%">
                            {precision:.1%}
                        </div>
                    </div>
                </div>
                
                <div style="margin: 20px 0;">
                    <strong>Recall: {recall:.2%}</strong>
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {recall*100}%">
                            {recall:.1%}
                        </div>
                    </div>
                </div>
                
                <div style="margin: 20px 0;">
                    <strong>F1 Score: {f1:.2%}</strong>
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {f1*100}%">
                            {f1:.1%}
                        </div>
                    </div>
                </div>
                
                <div style="margin: 20px 0;">
                    <strong>mAP@0.5: {results.get('map_50', 0):.2%}</strong>
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {results.get('map_50', 0)*100}%">
                            {results.get('map_50', 0):.1%}
                        </div>
                    </div>
                </div>
            </div>
        </div>
        """
        
    def _generate_per_class_section(self, results: Dict) -> str:
        """Generate per-class metrics section with detailed statistics."""
        per_class = results.get('per_class_metrics', {})
        metadata = results.get('metadata', {})
        class_names = metadata.get('class_names', {})
        
        # Include all classes, even those with no detections
        all_classes = set(class_names.values()) if class_names else set()
        detected_classes = set(per_class.keys())
        
        rows = []
        
        # First, show detected classes
        for class_name in sorted(detected_classes):
            metrics = per_class[class_name]
            precision = metrics.get('precision', 0)
            recall = metrics.get('recall', 0)
            ap = metrics.get('ap_50', 0)
            tp = metrics.get('true_positives', 0)
            fp = metrics.get('false_positives', 0)
            fn = metrics.get('false_negatives', 0)
            gt_count = metrics.get('ground_truth_count', 0)
            
            # Determine class for styling
            if ap >= 0.8:
                ap_class = "good"
            elif ap >= 0.5:
                ap_class = "warning"
            else:
                ap_class = "bad"
                
            # Status column
            status = []
            if tp > 0:
                status.append(f'<span class="good">✓ {tp} correct</span>')
            if fp > 0:
                status.append(f'<span class="warning">⚠ {fp} false</span>')
            if fn > 0:
                status.append(f'<span class="bad">✗ {fn} missed</span>')
            status_html = '<br>'.join(status) if status else '<span class="good">Perfect</span>'
                
            rows.append(f"""
                <tr>
                    <td><strong>{class_name}</strong></td>
                    <td>{gt_count}</td>
                    <td class="good">{tp}</td>
                    <td class="warning">{fp}</td>
                    <td class="bad">{fn}</td>
                    <td>{status_html}</td>
                    <td>{precision:.1%}</td>
                    <td>{recall:.1%}</td>
                    <td class="{ap_class}">{ap:.1%}</td>
                </tr>
            """)
        
        # Then show undetected classes if any
        undetected_classes = all_classes - detected_classes
        for class_name in sorted(undetected_classes):
            rows.append(f"""
                <tr style="opacity: 0.6;">
                    <td><strong>{class_name}</strong></td>
                    <td>0</td>
                    <td>0</td>
                    <td>0</td>
                    <td>0</td>
                    <td><span class="warning">No detections</span></td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                </tr>
            """)
            
        if not rows:
            return ""
            
        return f"""
        <div class="section">
            <div class="section-header">Per-Class Detection Performance</div>
            <table>
                <thead>
                    <tr>
                        <th>Class Name</th>
                        <th>Ground Truth</th>
                        <th>Correct</th>
                        <th>False Detections</th>
                        <th>Missed</th>
                        <th>Status Summary</th>
                        <th>Precision</th>
                        <th>Recall</th>
                        <th>AP@0.5</th>
                    </tr>
                </thead>
                <tbody>
                    {''.join(rows)}
                </tbody>
            </table>
            <p style="margin-top: 10px; font-size: 0.9em; color: #666;">
                <strong>Legend:</strong> 
                <span class="good">✓ Correct detections</span> | 
                <span class="warning">⚠ False positives</span> | 
                <span class="bad">✗ Missed detections</span>
            </p>
        </div>
        """
        
    def _generate_problem_images_section(self, results: Dict) -> str:
        """Generate section showing images with detection problems."""
        problem_images = results.get('problem_images', [])
        metadata = results.get('metadata', {})
        class_names = metadata.get('class_names', {})
        
        if not problem_images:
            return f"""
            <div class="section">
                <div class="section-header">Problem Images Analysis</div>
                <div class="recommendation good">
                    No problematic images found! All images have perfect or near-perfect detection.
                </div>
            </div>
            """
        
        # Limit to top 20 worst performing images
        worst_images = problem_images[:20]
        
        rows = []
        for img_data in worst_images:
            # Build problem details
            problem_details = []
            
            # Add false positive details
            if img_data['false_positives'] > 0:
                fp_classes = {}
                for fp in img_data.get('fp_details', []):
                    class_name = class_names.get(fp['class_id'], f"Class {fp['class_id']}")
                    conf = fp.get('confidence', 0)
                    if class_name not in fp_classes:
                        fp_classes[class_name] = []
                    fp_classes[class_name].append(conf)
                
                fp_text = []
                for cls, confs in fp_classes.items():
                    avg_conf = sum(confs) / len(confs)
                    fp_text.append(f"{cls} ({len(confs)}x, avg conf: {avg_conf:.1%})")
                problem_details.append(f'<span class="warning">False positives: {", ".join(fp_text)}</span>')
            
            # Add false negative details
            if img_data['false_negatives'] > 0:
                fn_classes = {}
                for fn in img_data.get('fn_details', []):
                    class_name = class_names.get(fn['class_id'], f"Class {fn['class_id']}")
                    if class_name not in fn_classes:
                        fn_classes[class_name] = 0
                    fn_classes[class_name] += 1
                
                fn_text = [f"{cls} ({count}x)" for cls, count in fn_classes.items()]
                problem_details.append(f'<span class="bad">Missed: {", ".join(fn_text)}</span>')
            
            # Determine severity
            f1 = img_data['f1_score']
            if f1 == 0:
                severity = "critical"
                severity_text = "Critical"
            elif f1 < 0.3:
                severity = "bad"
                severity_text = "Poor"
            elif f1 < 0.6:
                severity = "warning"
                severity_text = "Fair"
            else:
                severity = "warning"
                severity_text = "Minor Issues"
            
            rows.append(f"""
                <tr>
                    <td>{img_data['image']}</td>
                    <td class="{severity}">{severity_text}</td>
                    <td>{img_data['false_positives']}</td>
                    <td>{img_data['false_negatives']}</td>
                    <td>{img_data['precision']:.1%}</td>
                    <td>{img_data['recall']:.1%}</td>
                    <td>{f1:.1%}</td>
                    <td style="font-size: 0.9em;">{'<br>'.join(problem_details)}</td>
                </tr>
            """)
        
        additional_count = len(problem_images) - len(worst_images)
        additional_note = ""
        if additional_count > 0:
            additional_note = f"""
            <p style="margin-top: 15px; color: #666; font-style: italic;">
                Note: Showing {len(worst_images)} worst performing images out of {len(problem_images)} total problematic images.
            </p>
            """
        
        return f"""
        <div class="section">
            <div class="section-header">Problem Images Analysis</div>
            <p style="margin-bottom: 15px;">
                Images with detection issues (false positives or missed detections), sorted by worst performance first.
            </p>
            <table>
                <thead>
                    <tr>
                        <th>Image Name</th>
                        <th>Severity</th>
                        <th>False Pos</th>
                        <th>Missed</th>
                        <th>Precision</th>
                        <th>Recall</th>
                        <th>F1 Score</th>
                        <th>Problem Details</th>
                    </tr>
                </thead>
                <tbody>
                    {''.join(rows)}
                </tbody>
            </table>
            {additional_note}
        </div>
        """
        
    def _generate_per_image_section(self, results: Dict) -> str:
        """Generate per-image results section (collapsible)."""
        per_image = results.get('per_image_results', {})
        if not per_image:
            return ""
            
        rows = []
        for image_name, metrics in per_image.items():
            precision = metrics.get('precision', 0)
            recall = metrics.get('recall', 0)
            f1 = metrics.get('f1_score', 0)
            
            rows.append(f"""
                <tr>
                    <td>{image_name}</td>
                    <td>{metrics.get('ground_truth_count', 0)}</td>
                    <td>{metrics.get('prediction_count', 0)}</td>
                    <td>{metrics.get('true_positives', 0)}</td>
                    <td>{metrics.get('false_positives', 0)}</td>
                    <td>{metrics.get('false_negatives', 0)}</td>
                    <td>{precision:.2%}</td>
                    <td>{recall:.2%}</td>
                    <td>{f1:.2%}</td>
                    <td>{metrics.get('inference_time', 0):.3f}s</td>
                </tr>
            """)
            
        return f"""
        <div class="section">
            <button class="collapsible section-header" onclick="toggleCollapse(this)">
                Per-Image Results (Click to expand)
            </button>
            <div class="collapsible-content">
                <table>
                    <thead>
                        <tr>
                            <th>Image</th>
                            <th>GT</th>
                            <th>Pred</th>
                            <th>TP</th>
                            <th>FP</th>
                            <th>FN</th>
                            <th>Precision</th>
                            <th>Recall</th>
                            <th>F1</th>
                            <th>Time</th>
                        </tr>
                    </thead>
                    <tbody>
                        {''.join(rows)}
                    </tbody>
                </table>
            </div>
        </div>
        
        <script>
        function toggleCollapse(element) {{
            var content = element.nextElementSibling;
            if (content.style.display === "block") {{
                content.style.display = "none";
            }} else {{
                content.style.display = "block";
            }}
        }}
        </script>
        """
        
    def _generate_recommendations_section(self, results: Dict) -> str:
        """Generate recommendations based on metrics."""
        recommendations = []
        
        precision = results.get('precision', 0)
        recall = results.get('recall', 0)
        f1 = results.get('f1_score', 0)
        fp = results.get('false_positives', 0)
        fn = results.get('false_negatives', 0)
        
        # F1 score based recommendations
        if f1 >= 0.8:
            recommendations.append(("good", "Excellent overall performance! Model is production-ready."))
        elif f1 >= 0.6:
            recommendations.append(("good", "Good performance. Model is suitable for most applications."))
        elif f1 >= 0.4:
            recommendations.append(("warning", "Fair performance. Consider additional training or data augmentation."))
        else:
            recommendations.append(("critical", "Poor performance. Significant improvements needed."))
            
        # Precision vs Recall balance
        if precision < 0.5:
            recommendations.append(("critical", f"Low precision ({precision:.1%}): Too many false positives. Consider increasing confidence threshold."))
        elif precision < 0.7:
            recommendations.append(("warning", f"Moderate precision ({precision:.1%}): Some false positives present."))
            
        if recall < 0.5:
            recommendations.append(("critical", f"Low recall ({recall:.1%}): Missing many objects. Consider lowering confidence threshold or more training."))
        elif recall < 0.7:
            recommendations.append(("warning", f"Moderate recall ({recall:.1%}): Some objects being missed."))
            
        # False positives and negatives
        total = results.get('true_positives', 0) + fp + fn
        if total > 0:
            fp_rate = fp / total
            fn_rate = fn / total
            
            if fp_rate > 0.3:
                recommendations.append(("warning", f"High false positive rate ({fp_rate:.1%}). Model may need more negative examples."))
            if fn_rate > 0.3:
                recommendations.append(("warning", f"High false negative rate ({fn_rate:.1%}). Model may need more diverse training data."))
                
        # Inference time
        avg_time = results.get('avg_inference_time', 0)
        if avg_time > 1.0:
            recommendations.append(("warning", f"Slow inference ({avg_time:.2f}s per image). Consider model optimization for real-time use."))
        elif avg_time < 0.1:
            recommendations.append(("good", f"Fast inference ({avg_time:.3f}s per image). Suitable for real-time applications."))
            
        # Per-class performance
        per_class = results.get('per_class_metrics', {})
        poor_classes = [name for name, m in per_class.items() if m.get('ap_50', 0) < 0.5]
        if poor_classes:
            recommendations.append(("warning", f"Poor performance on classes: {', '.join(poor_classes[:5])}. Consider class-specific augmentation."))
            
        rec_html = []
        for rec_class, rec_text in recommendations:
            rec_html.append(f'<div class="recommendation {rec_class}">{rec_text}</div>')
            
        return f"""
        <div class="section">
            <div class="section-header">Recommendations</div>
            {''.join(rec_html)}
        </div>
        """